{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a9d3dd",
   "metadata": {},
   "source": [
    "### Importing all the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31e90aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube_transcript_api\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9436de",
   "metadata": {},
   "source": [
    "### Get the subtitles of the video Using Unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e79920",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.youtube.com/watch?v=7x5M4lxK-dw\" \n",
    "unique_id = link.split(\"=\")[-1]\n",
    "sub = YouTubeTranscriptApi.get_transcript(unique_id)  \n",
    "subtitle = \" \".join([x['text'] for x in sub])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa158742",
   "metadata": {},
   "source": [
    "### Summarization using TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f1228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF or term frequency-inverse document frequency is a vectorizer that converts the text into a vector.\n",
    "### It has 2 terms term frequency and inverse document frequency.\n",
    "### Term frequency is the number of repetitions of words in a sentence by the total number of words in that sentence.\n",
    "### Inverse document frequency is the log of no of sentences by the number of sentences containing the given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f1f6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using sentence_tokenizer of nltk library for tokenization.\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "subtitle = subtitle.replace(\"n\",\"\")\n",
    "sentences = sent_tokenize(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddefccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizing the tokenized sentences into the dictionary with the sentence as the key and corresponding index to its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "651c0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "organized_sent = {k:v for v,k in enumerate(sentences)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e99d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the tf-idf vectorizer,will get the scores of each sentence that we created during tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3604c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umang\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['w'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf_idf = TfidfVectorizer(min_df=1, \n",
    "                                    strip_accents='unicode',\n",
    "                                    max_features=None,\n",
    "                                    lowercase = True,\n",
    "                                    token_pattern=r'w{1,}',\n",
    "                                    ngram_range=(1, 3), \n",
    "                                    use_idf=1,\n",
    "                                    smooth_idf=1,\n",
    "                                    sublinear_tf=1,\n",
    "                                    stop_words = 'english')\n",
    " \n",
    "\n",
    "sentence_vectors = tf_idf.fit_transform(sentences)\n",
    "sent_scores = np.array(sentence_vectors.sum(axis=1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8654717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out the top N sentences that have a larger score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46b12b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "top_n_sentences = [sentences[index] for index in np.argsort(sent_scores, axis=0)[::-1][:N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca678e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let’s order the top sentences based on the order in the subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6d948cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the scored sentences with their indexes as in the subtitle\n",
    "mapped_sentences = [(sentence,organized_sent[sentence]) for sentence in top_n_sentences]\n",
    "# Ordering the top-n sentences in their original order\n",
    "mapped_sentences = sorted(mapped_sentences, key = lambda x: x[1])\n",
    "ordered_sentences = [element[0] for element in mapped_sentences]\n",
    "# joining the ordered sentence\n",
    "summary = \" \".join(ordered_sentences)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb1178",
   "metadata": {},
   "source": [
    "### Summarization using BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99fbb7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25ec3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07c795f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "# encode this subtitle using the Bart Tokenizer.\n",
    "input_tensor = tokenizer.encode( subtitle, return_tensors=\"pt\", max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df50e22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0, 25093,  4134,   324,    16,    65,     9,     5,   144,\n",
       "          5372,  5494,   710,  3958,     9,    70,    86,     4,  1614, 13448,\n",
       "          1090,    16,     5,  1482,     9,     5,    10,  1549,   329,  8591,\n",
       "             4, 20863,    16,     5,  2730,     9,     5,  1040,    22, 25093,\n",
       "           298,    18,  5972,     9,  2032, 15409,   113,     8,  1029,    12,\n",
       "         11438,     9,     5,   939, 20697,  8591,     4,    20,  1040,    16,\n",
       "           716,    15,     5,  1040,     9,     5,   276,   766,    30,  3259,\n",
       "         20269,     8,  1575,     5,   173,     9,  3259, 20269,     6,  3259,\n",
       "          1745,     6,     8,   643,     4,    20,  8591,    16,   577,    15,\n",
       "         14734,     8,    34,    10,   316,    12,  8596,  7425,     9,    70,\n",
       "             5,  7614,    24,    34,  2913,    98,   444,     4,    85,    16,\n",
       "            67,   577,    15,   312, 36519,     8,     5, 27879,  1121,  1553,\n",
       "            13,     5,  2733,     8,  9481,     4,   286,    55,   335,    15,\n",
       "             5,  8591,     6,   825,    10,  1549,  1301,     4,   175,    50,\n",
       "           213,     7, 50141,  1401,     4,   102,  1549,   329,     4,   175,\n",
       "             4,   286,     5,   455,  1194,    19, 20863,     6,   825, 50141,\n",
       "          1843,  1040,     9,  3553, 15409,     6, 50141, 25093,   298,    17,\n",
       "            27,    29,  5972,     9, 50141, 11329, 15409, 50141,   463, 50141,\n",
       "           118, 20697, 50141,   261, 50141, 14734,     8, 50141,  5320, 36519,\n",
       "             4, 50141,  2709,     5,   455, 12348,     9,     5,  1194,    19,\n",
       "         10694,     6,   825,    35, 50141,  8166,   640,  1401,     4,   620,\n",
       "         36519,     4,   175,    73,    29,    73, 25093,   298,    12, 25093,\n",
       "           298, 50141,    17,    48, 25093,   298,    16,   124,     4,    17,\n",
       "            46,     4,   286, 50141,   627,   455,  1732,     9,    42,  1566,\n",
       "             6,  2540,   213,     7,    35, 50141,    17,   711,  1401,     4,\n",
       "         13975,   298,    12, 13975,   298,     4,   175,    17,    27,     4,\n",
       "          1437, 50141,    17,    27,  1401,     4,  6526,  2987,    12, 13975,\n",
       "          4134,   324,    12,    17,    27, 21426,  2987,    17,    27, 50141,\n",
       "            17,    46, 21426,  2987,     4,    17,    27,    17,    27,    44,\n",
       "           711, 21426,  2987,     6,    17,    27,    15,     5,  3748,    17,\n",
       "            27,    35,    44,   711, 25093,   298,     6,    38,    17,    27,\n",
       "           119,   259,     7,  1067,     7,    47,    59,    10,    92,  1553,\n",
       "            14,    40,  1157, 34577,  2923,     7,  1296,    49,   162,   687,\n",
       "           634, 36323, 14284,     4,    85,    40,  1157,   106,     7,  1045,\n",
       "            10,  1296,  1023,  2257,    14, 34577,  2923,  6056, 23057,   990,\n",
       "          2422,  2773,     4,    85,    17,    27,   890,    28,   373,     5,\n",
       "         11212, 14414,    10,   741,  4272,     9, 34577,  2923,    33,   213,\n",
       "           242,  1021, 11527,  9740,  1717, 43948,    51,    33,     5, 36323,\n",
       "         14284,   235,  2329, 45420, 43377,    14,    38,  3553,   967,     9,\n",
       "           358, 17224,   459,    86,    25,    10,    24, 28290,   210,   254,\n",
       "            16,   596,    51,    32, 19313,    10,   741,  1296,  1023,    49,\n",
       "           162,  3048,  2329,    98,    38,  3553,   330,   103,  3540,   364,\n",
       "         12080,     7,   213,  2329,  1045,    10,   741, 21959,  1023,  2257,\n",
       "            17,    27,  1437, 50141, 21426,  2987,    16,     5,  3787,     9,\n",
       "             5,  8591,    44,   711,   250,  1549,   329,    17,    27,     8,\n",
       "            16,    67,     5,  1029,    12, 37652,     9,     5,   311,    44,\n",
       "           711,   133,    83,  1549,   329,  2907,    17,    27,     6,    61,\n",
       "         16468,    15,   186,   282,  6183,    23,   158,  1685,  4799,    15,\n",
       "             5,    83,  1549,  1301,  4238,     4,    91,    16,    67,    10,\n",
       "          1029,    12,  7295,     9,     5,   998, 12991, 32110,     4,   175,\n",
       "             8,    16,    10,  7690,  4910,    15,     5,   311,     4,    20,\n",
       "           311,    16,  4457,    30,  1960,     8,   162,     4,     2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_tensor = model.generate(input_tensor, max_length=1000, min_length=500,length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "outputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "217a81b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s><s>Stephanie is one of the most requested retur guests of all time. Adreesse is the host of the a16z podcast. Steph is the author of the book \"Steph's Book of Thigs\" and co-host of the iReport podcast. The book is based on the book of the same name by Stephen Colbert and features the work of Stephen Colbert, Stephen King, and others. The podcast is available on iTunes and has a 12-page log of all the topics it has covered so far. It is also available on Stitcher and the TuneIn app for the iPhone and iPad. For more information on the podcast, visit a16Z.com or go to www.a16z.com. For the full interview with Steph, visit her book of thigs, Steph’s Book of Thigs and iReport on  iTunes and Stitcher. For the full transcript of the interview with Stephanie, visit: http://www.stitcher.com/s/Steph-Steph “Steph is back.”. For the full version of this article, please go to: ‘www.steph-steph.com’.  ’www.steven-stephanie-’Steven’ ”Steven.’’ ‘Steven,’ on the web’: ‘Steph, I’m here to talk to you about a new app that will allow restaurats to test their meus using QR codes. It will allow them to create a testig software that restaurats ca igest super easily. It’ll be called the pademic a buch of restaurats have goe olie quote uquote they have the QR codes right ad somethig that I thik of every sigle time as a iteret marketer is why they are ot a b testig their meUS ad so I thk someoe eeds to go ad create a btestig software’  Steven is the founder of the podcast ‘A16z’ and is also the co-creator of the show ‘The A16z Show’, which airs on weeknights at 10pm ET on the A16Z channel. He is also a co-founder of the website HubSpot.com and is a frequent guest on the show. The show is hosted by Sam and me.</s>\n"
     ]
    }
   ],
   "source": [
    "# outputs will be a tensor in order to get text out of it, we need to decode it using the same Bart Tokenizer model.\n",
    "print(tokenizer.decode(outputs_tensor[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
